{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A working implementation of the AAE in Tensorflow as in the old GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vedang/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and test files\n",
    "\n",
    "train = 'train_aae.txt'\n",
    "test = 'test_aae.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```batch_gen``` generates the batches and randomly shuffles them and ```buffered_gen``` generates the buffer and uses it to store the batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gen(data, batch_n):\n",
    "    \"\"\"\n",
    "    Given the data, returns the batches using random shuffling.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.array\n",
    "        Consists of all the instances set into a numpy array\n",
    "    batch_n: int\n",
    "        Size of each batch\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    data: Generator object\n",
    "        A generator object with all the batches in it.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Create a list of indices for the data,\n",
    "    # which is of the same size as the data.\n",
    "    # For eg,\n",
    "    #     If data is of size 1000,\n",
    "    #     inds = [0, 1, ..., 999]\n",
    "    inds = list(range(data.shape[0]))\n",
    "    \n",
    "    # Randomly shuffle the indices.\n",
    "    # For eg,\n",
    "    #     inds = [650, 720, ..., 2]\n",
    "    np.random.shuffle(inds)\n",
    "    \n",
    "    # Generate size of data / batch size\n",
    "    # number of batches.\n",
    "    # For eg,\n",
    "    #     If data is of size 1000 and \n",
    "    #     batch_n = 50, then i will be\n",
    "    #     in range 0, 19\n",
    "    for i in range(int(data.shape[0] / batch_n)):\n",
    "        \n",
    "        # Split inds according to the batches\n",
    "        # created by i\n",
    "        ii = inds[i*batch_n:(i+1)*batch_n]\n",
    "        \n",
    "        # Return a generator with each index\n",
    "        # in ii matching that tuple from data.\n",
    "        # For eg,\n",
    "        #     If data = [[0, 1], [1, 2], ..., [999, 1000]]\n",
    "        #     and ii = [50, 55, 1] (and batch_n = 3)\n",
    "        #     then return [[50, 51], [55, 56], [1, 2]]\n",
    "        yield data[ii, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buffered_gen(f, batch_n=1024, buffer_size=2000):\n",
    "    \"\"\"\n",
    "    Creates the batches by reading a file 'f', where the\n",
    "    data is stored. \n",
    "    \n",
    "    The data is stored in a buffer.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    f: str\n",
    "        String containing address of file\n",
    "        \n",
    "    batch_n: int\n",
    "        Size of each batch\n",
    "        \n",
    "    buffer_size: int\n",
    "        Size of the buffer. Denotes total number\n",
    "        of batches which can be possibly stored in\n",
    "        the buffer.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A generator object with all the batches in it. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Open file f\n",
    "    inp = open(f)\n",
    "    \n",
    "    # Create new data list\n",
    "    data = []\n",
    "    \n",
    "    # i = index of line, line is the line read\n",
    "    for i, line in enumerate(inp):\n",
    "        \n",
    "        # For each line,\n",
    "        # the line is first stripped, and the split according to tabs\n",
    "        # the first element is read into an array and each input of it\n",
    "        # is converted to a float. This is then appended to data.\n",
    "        data.append(np.array(list(map(float, line.strip().split('\\t')[1]))))\n",
    "        \n",
    "        # If size of buffer is finished, that is the buffer can store\n",
    "        # data only uptill the number of instances = buffer_size * batch_n\n",
    "        # and if the next instance fills up the buffer, then...\n",
    "        if (i+1) % (buffer_size * batch_n) == 0:\n",
    "            \n",
    "            # Generate batches for whatever has been stored in data so far\n",
    "            bgen = batch_gen(np.vstack(data), batch_n)\n",
    "            \n",
    "            # Yield the batches\n",
    "            for batch in bgen:\n",
    "                yield batch\n",
    "            \n",
    "            # Empty data\n",
    "            data = []\n",
    "            \n",
    "    else:\n",
    "        # Generate batches while leaving out the last element of data\n",
    "        bgen = batch_gen(np.vstack(data[:-1]), batch_n)\n",
    "\n",
    "        # Yield the batches\n",
    "        for batch in bgen:\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the data from the test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test():\n",
    "    with open(test) as inp:\n",
    "        data = [np.array(list(map(float, line.strip().split('\\t')[1]))) for line in inp]\n",
    "    return np.vstack(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions for AAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_initializer(size):\n",
    "    return tf.random_normal_initializer(mean=0.0, stddev=np.sqrt(1. / size), seed=None, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer(tensor, input_size, out_size, init_fn=he_initializer,):\n",
    "    W = tf.get_variable('W', shape=[input_size, out_size], initializer=init_fn(input_size))\n",
    "    b = tf.get_variable('b', shape=[out_size], initializer=tf.constant_initializer(0.1))\n",
    "    return tf.add(tf.matmul(tensor, W), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_layer(tensor, size, epsilon=0.0001):\n",
    "    batch_mean, batch_var = tf.nn.moments(tensor, [0])\n",
    "    scale = tf.get_variable('scale', shape=[size], initializer=tf.constant_initializer(1.))\n",
    "    beta = tf.get_variable('beta', shape=[size], initializer=tf.constant_initializer(0.))\n",
    "    return tf.nn.batch_normalization(tensor, batch_mean, batch_var, beta, scale, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_prior(loc=0., scale=1., size=(64, 10)):\n",
    "    return np.random.normal(loc=loc, scale=scale, size=size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual implementation of the AAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAE(object):\n",
    "    def __init__(self,\n",
    "                 gpu_config=None,\n",
    "                 batch_size=1024, \n",
    "                 input_space=167,\n",
    "                 latent_space=20,\n",
    "                 middle_layers=None,\n",
    "                 activation_fn=tf.nn.tanh,\n",
    "                 learning_rate=0.001,\n",
    "                 initializer=he_initializer):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.input_space = input_space\n",
    "        self.latent_space = latent_space\n",
    "        if middle_layers is None:\n",
    "            self.middle_layers = [256, 256]\n",
    "        else:\n",
    "            self.middle_layers = middle_layers\n",
    "        self.activation_fn = activation_fn\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.initializer = initializer\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.input_x = tf.placeholder(tf.float32, [None, input_space])\n",
    "        self.z_tensor = tf.placeholder(tf.float32, [None, latent_space])\n",
    "\n",
    "        # Encoder net: 152->256->256->10\n",
    "        with tf.variable_scope(\"encoder\"):\n",
    "            self.encoder_layers = self.encoder()\n",
    "            self.encoded = self.encoder_layers[-1]\n",
    "        \n",
    "        # Decoder net: 10->256->256->152\n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            self.decoder_layers = self.decoder(self.encoded)\n",
    "            self.decoded = self.decoder_layers[-1]\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            self.generator_layers = self.decoder(self.z_tensor)\n",
    "            self.generated = tf.nn.sigmoid(self.generator_layers[-1])\n",
    "\n",
    "        # Discriminator net: 10->64->64->8->1\n",
    "        sizes = [64, 64, 8, 1]\n",
    "        with tf.variable_scope(\"discriminator\"):\n",
    "            self.disc_layers_neg = self.discriminator(self.encoded, sizes)\n",
    "            self.disc_neg = self.disc_layers_neg[-1]\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            self.disc_layers_pos = self.discriminator(self.z_tensor, sizes)\n",
    "            self.disc_pos = self.disc_layers_pos[-1]\n",
    "\n",
    "        self.pos_loss = tf.nn.relu(self.disc_pos) - self.disc_pos + tf.log(1.0 + tf.exp(-tf.abs(self.disc_pos)))\n",
    "        self.neg_loss = tf.nn.relu(self.disc_neg) + tf.log(1.0 + tf.exp(-tf.abs(self.disc_neg)))\n",
    "        self.disc_loss = tf.reduce_mean(tf.add(self.pos_loss, self.neg_loss))\n",
    "        \n",
    "        tf.summary.scalar(\"discriminator_loss\", self.disc_loss)\n",
    "            \n",
    "        self.enc_loss = tf.reduce_mean(tf.nn.relu(self.disc_neg) - self.disc_neg + tf.log(1.0 + tf.exp(-tf.abs(self.disc_neg))))\n",
    "        batch_logloss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.decoded, labels=self.input_x), 1)\n",
    "        self.dec_loss = tf.reduce_mean(batch_logloss)\n",
    "        \n",
    "        tf.summary.scalar(\"encoder_loss\", self.enc_loss)\n",
    "        tf.summary.scalar(\"decoder_loss\", self.dec_loss)\n",
    "        \n",
    "        disc_ws = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='discriminator')\n",
    "        enc_ws = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='encoder')\n",
    "        ae_ws = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='encoder') + \\\n",
    "                tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='decoder')\n",
    "            \n",
    "        self.train_discriminator = tf.train.AdamOptimizer(self.learning_rate).minimize(self.disc_loss, var_list=disc_ws)\n",
    "        self.train_encoder = tf.train.AdamOptimizer(self.learning_rate).minimize(self.enc_loss, var_list=enc_ws)\n",
    "        self.train_autoencoder = tf.train.AdamOptimizer(self.learning_rate).minimize(self.dec_loss, var_list=ae_ws)\n",
    "\n",
    "        if gpu_config is None:\n",
    "            gpu_config = tf.ConfigProto()\n",
    "            gpu_config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "        \n",
    "        self.sess = tf.Session(config=gpu_config)\n",
    "        self.init_net()\n",
    "        \n",
    "    def encoder(self):\n",
    "        sizes = self.middle_layers + [self.latent_space]\n",
    "        with tf.variable_scope(\"layer-0\"):\n",
    "            encoder_layers = [linear_layer(self.input_x, self.input_space, sizes[0])]\n",
    "        for i in range(len(sizes) - 1):\n",
    "            with tf.variable_scope(\"layer-%i\" % (i+1)):\n",
    "                activated = self.activation_fn(encoder_layers[-1])\n",
    "#                 normed = bn_layer(activated, sizes[i])\n",
    "                next_layer = linear_layer(activated, sizes[i], sizes[i+1])\n",
    "            encoder_layers.append(next_layer)\n",
    "            \n",
    "        return encoder_layers\n",
    "\n",
    "    def decoder(self, tensor):\n",
    "        sizes = self.middle_layers[::-1] + [self.input_space]\n",
    "        with tf.variable_scope(\"layer-0\"):\n",
    "            decoder_layers = [linear_layer(tensor, self.latent_space, sizes[0])]\n",
    "        for i in range(len(sizes) - 1):\n",
    "            with tf.variable_scope(\"layer-%i\" % (i+1)):\n",
    "                activated = self.activation_fn(decoder_layers[-1])\n",
    "#                 normed = bn_layer(activated, sizes[i])\n",
    "                next_layer = linear_layer(activated, sizes[i], sizes[i+1])\n",
    "            decoder_layers.append(next_layer)\n",
    "        \n",
    "        return decoder_layers\n",
    "    \n",
    "    def discriminator(self, tensor, sizes):\n",
    "        with tf.variable_scope(\"layer-0\"):\n",
    "            disc_layers = [linear_layer(tensor, self.latent_space, sizes[0])]\n",
    "        for i in range(len(sizes) - 1):\n",
    "            with tf.variable_scope(\"layer-%i\" % (i+1)):\n",
    "                activated = tf.nn.tanh(disc_layers[-1])\n",
    "#                 normed = bn_layer(activated, sizes[i])\n",
    "                next_layer = linear_layer(activated, sizes[i], sizes[i+1])\n",
    "            disc_layers.append(next_layer)\n",
    "\n",
    "        return disc_layers\n",
    "    \n",
    "    def init_net(self):\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)        \n",
    "    \n",
    "    def train(self, log):\n",
    "        sess = self.sess\n",
    "        saver = tf.train.Saver()\n",
    "        hist = []\n",
    "        test_data = load_test()\n",
    "        \n",
    "        merged_summary = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter('/tmp/aae/1')\n",
    "        train_writer.add_graph(sess.graph)\n",
    "        \n",
    "        for e in tqdm(range(5)):\n",
    "            print (log, \"epoch #%d\" % (e+1))\n",
    "            log.flush()\n",
    "            train_gen = buffered_gen(train, batch_n=self.batch_size)\n",
    "            for i, batch_x in enumerate(train_gen):\n",
    "                if i%3 == 0:\n",
    "                    batch_z = sample_prior(scale=1.0, size=(len(batch_x), self.latent_space))\n",
    "                    sess.run(self.train_discriminator, feed_dict={self.input_x: batch_x, self.z_tensor: batch_z})\n",
    "                elif i%3 == 1:\n",
    "                    sess.run(self.train_encoder, feed_dict={self.input_x: batch_x})\n",
    "                else:\n",
    "                    sess.run(self.train_autoencoder, feed_dict={self.input_x: batch_x})\n",
    "                if i % 5 == 0:\n",
    "                    s = sess.run(merged_summary, feed_dict={self.input_x: test_data, self.z_tensor: batch_z})\n",
    "                    train_writer.add_summary(s, i)\n",
    "                if i%10000 == 0:\n",
    "                    batch_z = sample_prior(scale=1.0, size=(len(test_data), self.latent_space))\n",
    "                    \n",
    "                    losses = sess.run([merged_summary, self.disc_loss, self.enc_loss, self.dec_loss],\n",
    "                                      feed_dict={self.input_x: test_data, self.z_tensor: batch_z})\n",
    "                    discriminator_loss, encoder_loss, decoder_loss = losses\n",
    "                    print (log, \"disc: %f, encoder : %f, decoder : %f\" % (discriminator_loss/2., encoder_loss, decoder_loss)\n",
    "                    \n",
    "                    log.flush()\n",
    "            else:         \n",
    "                saver.save(sess, './fpt.aae.%de.model.ckpt' % e)\n",
    "                batch_z = sample_prior(scale=1.0, size=(len(test_data), self.latent_space))\n",
    "                losses = sess.run([merged_summary, self.disc_loss, self.enc_loss, self.dec_loss],\n",
    "                                  feed_dict={self.input_x: test_data, self.z_tensor: batch_z})\n",
    "\n",
    "\n",
    "                discriminator_loss, encoder_loss, decoder_loss = losses\n",
    "                print (log, \"disc: %f, encoder : %f, decoder : %f\" % (discriminator_loss/2., encoder_loss, decoder_loss))\n",
    "                \n",
    "#                 train_writer.add_summary(discriminator_loss, e)\n",
    "#                 train_writer.add_summary(encoder_loss, e)\n",
    "#                 train_writer.add_summary(decoder_loss, e)\n",
    "                    \n",
    "                log.flush()\n",
    "                hist.append(decoder_loss)\n",
    "        return hist\n",
    "    \n",
    "    def load(self, model):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self.sess, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "aae = AAE(batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='./fpt.aae.log' mode='w' encoding='UTF-8'> epoch #1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Fetch argument None has invalid type <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-099dd31e758a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./fpt.aae.log'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m      \u001b[0maae_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-e0f445eca3ba>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, log)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                     losses = sess.run([merged_summary, self.disc_loss, self.enc_loss, self.dec_loss],\n\u001b[0;32m--> 147\u001b[0;31m                                       feed_dict={self.input_x: test_data, self.z_tensor: batch_z})\n\u001b[0m\u001b[1;32m    148\u001b[0m                     \u001b[0mdiscriminator_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"disc: %f, encoder : %f, decoder : %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiscriminator_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1120\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \"\"\"\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \"\"\"\n\u001b[1;32m    351\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \"\"\"\n\u001b[1;32m    351\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m       raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n\u001b[0;32m--> 242\u001b[0;31m                                                                  type(fetch)))\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Fetch argument None has invalid type <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "with open('./fpt.aae.log', 'w') as log:\n",
    "     aae_0 = aae.train(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation of new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85399, 167)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = load_test()\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = buffered_gen(test, batch_n=aae.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x = test_gen.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
       "       0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
       "       0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = aae.sess.run(aae.encoded, feed_dict={aae.input_x: test_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85399, 20)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = aae.sess.run(aae.decoded, feed_dict={aae.input_x: batch_x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 167)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.56642634,  0.28335452,  1.2258128 , -1.016491  ,  0.22021618,\n",
       "       -1.0452902 ,  2.7479956 , -0.64809203,  1.410975  , -0.4760045 ,\n",
       "       -0.6511922 ,  0.6436478 , -1.0777869 ,  0.51563567,  1.1963315 ,\n",
       "       -1.9459113 ,  1.2421366 , -0.10109315,  0.27510852,  0.47127083,\n",
       "        0.84680563,  0.76023924,  0.388485  ,  0.27548635,  0.6691807 ,\n",
       "        0.43580619,  0.48733127,  0.12854216, -0.52711153, -0.11974109,\n",
       "       -1.1006262 ,  0.47382525,  0.8838354 , -0.75252366,  1.709012  ,\n",
       "        0.9474926 ,  1.3475755 , -0.846265  , -0.10852735,  0.5217573 ,\n",
       "       -0.26282737, -0.02629023, -2.3594286 , -0.09638367, -1.2328621 ,\n",
       "        0.06122294,  0.97818846,  1.0580148 ,  0.8119556 ,  1.5019103 ,\n",
       "       -0.49760845,  1.4860634 , -0.40076366,  0.93247914,  1.4257379 ,\n",
       "       -0.06022765,  0.21340919, -1.5957376 , -0.3102171 ,  0.1283986 ,\n",
       "        0.12628858,  1.0650078 , -0.49054572, -0.6597627 ,  0.22693285,\n",
       "       -1.0586512 ,  0.621275  , -0.11848655, -0.084423  ,  0.16049303,\n",
       "        0.95230687,  0.3419496 ,  1.1218885 , -0.28020906,  1.2937782 ,\n",
       "       -0.36275914,  0.7579065 , -0.81007403, -0.10222379,  0.21757114,\n",
       "        0.7336414 ,  0.09910422,  0.657918  , -0.43325403, -0.29610786,\n",
       "       -0.8151052 ,  1.1217471 ,  1.8519763 ,  1.4337593 , -0.40656278,\n",
       "        0.3768255 ,  0.50034565, -0.9862952 , -2.230385  ,  0.3047242 ,\n",
       "       -1.3866405 ,  0.83358425,  0.7046154 , -1.0300791 , -0.11879175,\n",
       "        0.42167136, -0.81480694, -1.3879905 , -0.4940733 , -0.17003962,\n",
       "        0.4061009 ,  1.3063024 ,  0.8674603 , -0.16378808, -1.0709838 ,\n",
       "       -0.7582696 , -0.23729393,  1.4727902 , -0.16094705, -0.7823168 ,\n",
       "        0.3670601 ,  0.12536465,  0.48675534,  1.2609454 ,  0.22520676,\n",
       "       -1.0470185 , -0.04728591, -0.37667742,  1.1466027 , -0.9197637 ,\n",
       "       -1.5796207 , -0.5528949 ,  0.9161723 , -0.61510116, -0.49969622,\n",
       "        0.02492178, -0.5623095 , -0.05052149, -0.5483105 , -0.77965176,\n",
       "       -2.0481694 ,  0.01560891, -0.7103361 ,  0.9232407 ,  0.9722497 ,\n",
       "       -0.5939314 ,  0.6366867 , -0.28407544,  0.9743831 ,  2.1076758 ,\n",
       "       -0.09107586, -0.21589863,  1.0880377 ,  0.3018031 ,  1.0882473 ,\n",
       "       -1.1965216 ,  0.23461974,  0.7098199 ,  1.6763407 , -0.6626184 ,\n",
       "        0.0289174 ,  1.3119563 ,  1.4063995 ,  0.46180767,  0.26359195,\n",
       "       -0.31093287,  0.19442806,  2.4607635 , -0.3460698 , -0.7067081 ,\n",
       "       -0.7018207 , -0.76403105], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = dec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.116636686"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for a in samp:\n",
    "    if a <= -0.25 or (a >= 0.25 and a <= 0.75) or a >= 1.25:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for a in aae.sess.run(tf.trainable_variables()):\n",
    "    total += np.size(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234060"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-381a6fe07b71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'total' is not defined"
     ]
    }
   ],
   "source": [
    "for value in :\n",
    "    print (value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
